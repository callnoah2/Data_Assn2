{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City/Number of tweets:\n",
      "Georgia: 2 tweets\n",
      "Doha: 2 tweets\n",
      "Dubai: 1 tweets\n",
      "Muscat: 1 tweets\n",
      "Mollet del Vallès: 1 tweets\n",
      "Zouagha: 1 tweets\n",
      "Stockholm: 1 tweets\n",
      "Al Rayyan: 1 tweets\n",
      "\n",
      "Country/Number of tweets:\n",
      "Qatar: 3 tweets\n",
      "USA: 2 tweets\n",
      "United Arab Emirates: 1 tweets\n",
      "Oman: 1 tweets\n",
      "España: 1 tweets\n",
      "Royaume du Maroc: 1 tweets\n",
      "Sweden: 1 tweets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_tweets(json_file_path):\n",
    "    # Read JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Dictionary to store the count of tweets per city and country\n",
    "    city_count = {}\n",
    "    country_count = {}\n",
    "\n",
    "    # Iterate through tweets\n",
    "    for tweet in data:\n",
    "        place = tweet.get('place', {})\n",
    "        if place:\n",
    "            full_name = place.get('full_name', None)\n",
    "\n",
    "            # Check if 'full_name' is present before extracting city and country\n",
    "            if full_name:\n",
    "                city, country = map(str.strip, full_name.split(','))\n",
    "\n",
    "                # Update city count\n",
    "                if city:\n",
    "                    city_count[city] = city_count.get(city, 0) + 1\n",
    "\n",
    "                # Update country count\n",
    "                if country:\n",
    "                    country_count[country] = country_count.get(country, 0) + 1\n",
    "\n",
    "    # Print results in descending order\n",
    "    print(\"City/Number of tweets:\")\n",
    "    for city, count in sorted(city_count.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{city}: {count} tweets\")\n",
    "\n",
    "    print(\"\\nCountry/Number of tweets:\")\n",
    "    for country, count in sorted(country_count.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{country}: {count} tweets\")\n",
    "\n",
    "\n",
    "json_file_path = \"./Data/FIFAWorldCup2022.json\"\n",
    "process_tweets(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "### PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "                      id               name     screen_name  \\\n",
      "0    1236629541327495169     Tasnim Sultana  tasnimmuna2610   \n",
      "1    1602456658344542208      sajad heidary   sajadheidary4   \n",
      "2             1798659276       Junyuan Hong          hjy836   \n",
      "3              102933899           Jennifer     EstiZhafira   \n",
      "4    1265285662489468928              scoji          scoji3   \n",
      "..                   ...                ...             ...   \n",
      "210              8143682      Jure Leskovec            jure   \n",
      "211             62044012  Michael Bronstein     mmbronstein   \n",
      "212   850892377627742209         Tyler Derr   TylersNetwork   \n",
      "213  1190461842922995712          CIKM 2021        CIKM2021   \n",
      "214             13334762             GitHub          github   \n",
      "\n",
      "    follower_or_followee                    location  \\\n",
      "0               follower                  Bangladesh   \n",
      "1               follower                       Eywan   \n",
      "2               follower                         USA   \n",
      "3               follower                     Tiburon   \n",
      "4               follower                               \n",
      "..                   ...                         ...   \n",
      "210             followee                Stanford, CA   \n",
      "211             followee             London, England   \n",
      "212             followee          Nashville, TN, USA   \n",
      "213             followee  Gold Coast, QLD, Australia   \n",
      "214             followee           San Francisco, CA   \n",
      "\n",
      "                                           description  followers_count  \\\n",
      "0             Looking for MS Assistantships in the USA                1   \n",
      "1    فارغ التحصیل مهندسی سخت افزار،فارغ التحصیل مهن...                0   \n",
      "2    I'm a Ph.D. candidate at Michigan State Univ a...               91   \n",
      "3    It's one thing to be hard to do, and it's anot...               51   \n",
      "4                                                                    27   \n",
      "..                                                 ...              ...   \n",
      "210  Professor of #computerscience @Stanford; Co-fo...            38221   \n",
      "211  #DeepMind Professor @UniofOxford / Fellow @Exe...            32816   \n",
      "212  Assistant Professor @Vanderbilt_CS; Network an...             1127   \n",
      "213     ACM CIKM 2021 Conference\\n1-5 Nov 2021, Online              828   \n",
      "214  The complete developer platform to build, scal...          2452420   \n",
      "\n",
      "     friends_count  favourites_count       creation_time  statuses_count  \\\n",
      "0               31                 2 2020-03-08 12:27:33               1   \n",
      "1               12                 0 2022-12-13 00:13:54               1   \n",
      "2             1132               248 2013-09-08 02:58:04             113   \n",
      "3             1224               915 2010-01-08 09:12:05               4   \n",
      "4             2818                72 2020-05-26 14:16:39               0   \n",
      "..             ...               ...                 ...             ...   \n",
      "210            349               773 2007-08-12 18:14:57            1208   \n",
      "211           3367             34030 2009-08-01 14:32:57            5158   \n",
      "212            984              3897 2017-04-09 02:05:26            1124   \n",
      "213           3076               126 2019-11-02 02:53:37             124   \n",
      "214            334              7681 2008-02-11 04:41:50            7883   \n",
      "\n",
      "     verified  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "..        ...  \n",
      "210     False  \n",
      "211     False  \n",
      "212     False  \n",
      "213     False  \n",
      "214      True  \n",
      "\n",
      "[215 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def create_dataframe(data, follower_or_followee):\n",
    "    df = pd.DataFrame(data)\n",
    "    df['follower_or_followee'] = follower_or_followee\n",
    "    df['creation_time'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "    df = df[['id', 'name', 'screen_name', 'follower_or_followee', 'location', 'description',\n",
    "             'followers_count', 'friends_count', 'favourites_count', 'creation_time',\n",
    "             'statuses_count', 'verified']]\n",
    "    return df\n",
    "\n",
    "# File paths\n",
    "followers_file_path = \"./Data/followers.json\"\n",
    "followees_file_path = \"./Data/followees.json\"\n",
    "\n",
    "# Load data\n",
    "followers_data = load_data(followers_file_path)\n",
    "followees_data = load_data(followees_file_path)\n",
    "\n",
    "# Create DataFrames\n",
    "followers_df = create_dataframe(followers_data, 'follower')\n",
    "followees_df = create_dataframe(followees_data, 'followee')\n",
    "\n",
    "# Concatenate DataFrames\n",
    "result_df = pd.concat([followers_df, followees_df], ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"Part A\")\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I) Average followers count of your followers: 2983.21\n",
      "II) Average followers count of your followees: 1075218.86\n",
      "III) Average followees count of your followers: 1055.49\n",
      "IV) Average followees count of your followees: 1019.7\n",
      "V) Number of your verified followers: 1\n",
      "VI) Average favorites count of your followers: 1899.68\n",
      "VII) Average number of tweets of your followers: 654.21\n",
      "VIII) Number of your followers per year:\n",
      "creation_year\n",
      "2008     1\n",
      "2009     3\n",
      "2010     4\n",
      "2011     5\n",
      "2012     4\n",
      "2013     3\n",
      "2014     4\n",
      "2015     3\n",
      "2016     7\n",
      "2017     4\n",
      "2018     6\n",
      "2019     9\n",
      "2020    13\n",
      "2021    13\n",
      "2022    12\n",
      "dtype: int64\n",
      "IX) Number of your followees per year:\n",
      "creation_year\n",
      "2007     4\n",
      "2008     5\n",
      "2009    16\n",
      "2010     8\n",
      "2011     9\n",
      "2012     9\n",
      "2013     6\n",
      "2014     6\n",
      "2015     8\n",
      "2016     8\n",
      "2017     8\n",
      "2018     3\n",
      "2019    10\n",
      "2020    10\n",
      "2021     8\n",
      "2022     6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "\n",
    "# Compute metrics\n",
    "average_followers_count_followers = round(followers_df['followers_count'].mean(), 2)\n",
    "average_followers_count_followees = round(followees_df['followers_count'].mean(), 2)\n",
    "average_followees_count_followers = round(followers_df['friends_count'].mean(), 2)\n",
    "average_followees_count_followees = round(followees_df['friends_count'].mean(), 2)\n",
    "\n",
    "num_verified_followers = followers_df[followers_df['verified'] == True].shape[0]\n",
    "num_verified_followees = followees_df[followees_df['verified'] == True].shape[0]\n",
    "num_verified_combined = result_df[result_df['verified'] == True].shape[0]\n",
    "\n",
    "average_favorites_count_followers = round(followers_df['favourites_count'].mean(), 2)\n",
    "average_favorites_count_followees = round(followees_df['favourites_count'].mean(), 2)\n",
    "average_favorites_count_combined = round(result_df['favourites_count'].mean(), 2)\n",
    "\n",
    "average_statuses_count_followers = round(followers_df['statuses_count'].mean(), 2)\n",
    "average_statuses_count_followees = round(followees_df['statuses_count'].mean(), 2)\n",
    "average_statuses_count_combined = round(result_df['statuses_count'].mean(), 2)\n",
    "\n",
    "# Extract the number of followers and followees per year\n",
    "result_df['creation_year'] = result_df['creation_time'].dt.year\n",
    "followers_per_year = result_df[result_df['follower_or_followee'] == 'follower'].groupby('creation_year').size()\n",
    "followees_per_year = result_df[result_df['follower_or_followee'] == 'followee'].groupby('creation_year').size()\n",
    "\n",
    "# Display the computed metrics\n",
    "print(\"I) Average followers count of your followers:\", average_followers_count_followers)\n",
    "print(\"II) Average followers count of your followees:\", average_followers_count_followees)\n",
    "print(\"III) Average followees count of your followers:\", average_followees_count_followers)\n",
    "print(\"IV) Average followees count of your followees:\", average_followees_count_followees)\n",
    "print(\"V) Number of your verified followers:\", num_verified_followers)\n",
    "print(\"VI) Average favorites count of your followers:\", average_favorites_count_followers)\n",
    "print(\"VII) Average number of tweets of your followers:\", average_statuses_count_followers)\n",
    "print(\"VIII) Number of your followers per year:\")\n",
    "print(followers_per_year)\n",
    "print(\"IX) Number of your followees per year:\")\n",
    "print(followees_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)\n",
      "Scraping failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9q/yldjq0js2ng8cysrh56tt9xw0000gn/T/ipykernel_66556/2280067102.py\", line 17, in scrape_projects\n",
      "    connection.request(\"GET\", path)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1282, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1328, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1277, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1037, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 975, in send\n",
      "    self.connect()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1454, in connect\n",
      "    self.sock = self._context.wrap_socket(self.sock,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1075, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1346, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import http.client\n",
    "import traceback\n",
    "\n",
    "def scrape_projects(url):\n",
    "    # Parse the URL to get the host and path\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "\n",
    "    try:\n",
    "        # Create an HTTP connection without SSL verification\n",
    "        connection = http.client.HTTPSConnection(host)\n",
    "\n",
    "        # Send the GET request\n",
    "        connection.request(\"GET\", path)\n",
    "\n",
    "        # Get the response\n",
    "        response = connection.getresponse()\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status == 200:\n",
    "            # Read and decode the response content\n",
    "            content = response.read().decode(\"utf-8\")\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Find all project containers\n",
    "            project_containers = soup.find_all('div', class_='project-container')\n",
    "\n",
    "            # List to store project information\n",
    "            projects = []\n",
    "\n",
    "            # Iterate through project containers\n",
    "            for container in project_containers:\n",
    "                # Extract title and description\n",
    "                title = container.find('h3').text.strip()\n",
    "                description = container.find('p').text.strip()\n",
    "\n",
    "                # Extract list of areas\n",
    "                areas_list = [area.text.strip() for area in container.find_all('li')]\n",
    "\n",
    "                # Append the project information to the projects list\n",
    "                projects.append((title, description, areas_list))\n",
    "\n",
    "            return projects\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content. Status code: {response.status}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for debugging\n",
    "        return None\n",
    "\n",
    "# URL of the research projects page\n",
    "url = \"https://cs.usu.edu/people/HamidKarimi/projects.html\"\n",
    "\n",
    "# Scrape projects\n",
    "result = scrape_projects(url)\n",
    "\n",
    "# Display the result\n",
    "if result:\n",
    "    for i, (title, description, areas_list) in enumerate(result, start=1):\n",
    "        print(f\"Project {i}:\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        print(f\"Areas: {areas_list}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Scraping failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
